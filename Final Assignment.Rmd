---
title: "Relating air pollution problems to U.S. health care systems."
author: "Akhil Kristamsetty"
date: '2023-12-05'
output:
  html_document: default
  word_document: default
---

## Introduction

This air quality analysis project examines temporal trends, categorical distribution, and predictive modeling for an inclusive data set. Initially, the visualizations reveal the general AQI pattern for various years. Then follows our examination of AQI’s category distribution in a particular year, revealing the occurrence of different air qualities. In addition, predictive modeling uses regressions and SVMs to estimate the prevalence of unhealthy air days. Collectively, these analyses are designed to understand how the dynamics of ambient conditions relate to the accuracy with which air quality is predicted by models developed for this purpose.

## Question 1: Overall AQI trend over the years
This graph shows the AQI peaks experienced throughout these years: 2021, 2022, and 2023. Blue lines that indicate the highest recorded AQI during given days are connected across each year by the red dots. Year is on the x-axis, and maximum AQI is measured on the y-axis. This is a chart designed to show how the annual max AQI has gone up and down through the given years, with particular emphasis on this year.
```{r}
df <- read.csv('Data.csv')


# Question 1: Overall AQI trend over the years
library(ggplot2)

# Subset the data for relevant columns and years
df_q1 <- df[, c("Year", "Days.with.AQI", "Max.AQI")]
df_q1 <- df_q1[df_q1$Year %in% c(2021, 2022, 2023), ]

# Plotting
ggplot(df_q1, aes(x = Year, y = `Max.AQI`, group = 1)) +
  geom_line(color = "blue") +
  geom_point(color = "red", size = 3) +
  labs(title = "Overall AQI Trend Over the Years",
       x = "Year", y = "Max AQI") +
  theme_minimal()

```

### Question 2: Distribution of AQI categories in 2023
The chart shows various AQI category distributions for the year 2023. The bars show the number of those days in every category, using color to represent each particular AQI one. The “Moderate” type has the longest bar, indicating that it is represented by a greater number of days with moderately good air quality. On the other hand, the “Hazardous” column has its shortest bar for the least days under hazardous air pollution forecasts in 2023.


```{r}
# Question 2: Distribution of AQI categories in 2023
library(ggplot2)

# Subset the data for relevant columns and the year 2023
df_q2 <- df[df$Year == 2023, c("Good.Days", "Moderate.Days", 
                               "Unhealthy.for.Sensitive.Groups.Days", 
                               "Unhealthy.Days", "Very.Unhealthy.Days", 
                               "Hazardous.Days")]

# Reshape the data for plotting
df_q2 <- tidyr::gather(df_q2, key = "Category", value = "Days", -1)

# Plotting with vertical x-axis labels
ggplot(df_q2, aes(x = Category, y = Days, fill = Category)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of AQI Categories in 2023",
       x = "AQI Category", y = "Number of Days") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

#Question 2: Can we identify a correlation between the number of days with high CO levels and high NO2 levels?
# Filter data for the specific columns and relevant year
df_gas_corr <- df[df$Year == 2023, c("Year", "Days.CO", "Days.NO2")]

# Create a scatter plot to visualize the correlation
ggplot(df_gas_corr, aes(x = `Days.CO`, y = `Days.NO2`, color = Year)) +
  geom_point() +
  labs(title = "Correlation Between Days with High CO and NO2 Levels",
       x = "Days with CO",
       y = "Days with NO2") +
  theme_minimal()

```
# Question 3 How does the number of unhealthy days vary across different Core-Based Statistical Areas (CBSAs)?



```{r}
# Load necessary libraries
library(dplyr)
library(plotly)

# Sample data
df_map <- df[, c("CBSA.Code", "Unhealthy.Days")]

# Assuming you have a data frame with CBSA codes and corresponding Unhealthy Days

# Create a map using Plotly
plot_ly(data = df_map, type = 'scattergeo', locationmode = 'USA-states',
        lon = ~CBSA.Code, lat = ~Unhealthy.Days,
        text = ~paste('CBSA Code: ', CBSA.Code, '<br>Unhealthy Days: ', Unhealthy.Days),
        mode = 'markers', marker = list(size = 10, opacity = 0.8)) %>%
  layout(title = 'Unhealthy Days by CBSA Code',
         geo = list(scope = 'usa', showland = TRUE))




```

## Question 4: Can we predict the number of Unhealthy Days based on other air quality parameters?
# Load necessary libraries
Use a regression mode to predict the number of “Unhealthy Days” from different air quality parameters. The Poisson regression using glmnet is employed. To quantify the model’s accuracy, the mean square error (MSE) was computed and resulted in about 1.809 values. The model is further assessed for binary classification by applying the threshold of 0.5. This produces an accuracy of 47.4%; thus, a moderate performance prediction is obtained.

```{r}
# Question 4: Can we predict the number of Unhealthy Days based on other air quality parameters?
# Load necessary libraries
library(caret)
library(glmnet)

# Select relevant columns
df_model <- df[, c("Days.CO", "Days.NO2", "Days.Ozone", "Days.PM2.5", "Days.PM10", "Unhealthy.Days")]

# Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(df_model$`Unhealthy.Days`, p = 0.8, list = FALSE)
train_data <- df_model[train_index, ]
test_data <- df_model[-train_index, ]

# Train a regression model
model <- glm(`Unhealthy.Days` ~ ., data = train_data, family = "poisson")

# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "response")

# Evaluate the model performance
mse <- mean((test_data$`Unhealthy.Days` - predictions)^2)
print(paste("Mean Squared Error:", mse))
# Set a threshold for binary classification (you can adjust this threshold)
threshold <- 0.5

# Convert predictions to binary values based on the threshold
binary_predictions <- ifelse(predictions > threshold, 1, 0)

# Create a binary response variable for the test set
binary_actual_values <- ifelse(test_data$`Unhealthy.Days` > threshold, 1, 0)

# Calculate accuracy for binary classification
accuracy <- sum(binary_predictions == binary_actual_values) / length(binary_actual_values)

# Print accuracy
print(paste("Accuracy for Regression Model (Binary Classification):", round(accuracy, 3)))

```

## Question 5 How well does the Support Vector Machine (SVM) model predict the binary health status (Unhealthy or Healthy) based on air quality parameters?
SVM binary classification model on the basis of binary health status using Linear kernel has performed excellently. The test has no false positives and no false negatives, which implies perfect accuracy, sensitivity, and specificity for the confusion matrix. This shows that the SVM model was effective in classifying instances of the health status and was better than the previous regression model that had an accuracy of 47.4%. A 100% balance accuracy in regard to discrimination between Unhealthy and Healthy categories with respect to the SVM model was determined.
```{r}
# Load necessary libraries
library(caret)
library(e1071)

# Select relevant columns with corrected names
df_svm <- df[, c("Year", "Days.CO", "Days.NO2", "Days.Ozone", "Days.PM2.5", "Days.PM10", "Unhealthy.Days")]
# Rename columns with spaces
colnames(df_svm) <- c("Year", "Days_CO", "Days_NO2", "Days_Ozone", "Days_PM2.5", "Days_PM10", "Unhealthy_Days")
colnames(df_svm)
# Create a binary classification variable: Unhealthy (1) or Healthy (0)
df_svm$HealthStatus <- ifelse(df_svm$Unhealthy_Days > 0, 1, 0)
head(df_svm)
# Set factor levels explicitly
df_svm$HealthStatus <- factor(df_svm$HealthStatus, levels = c(0, 1))

# Split the data into training and testing sets
set.seed(123)
train_index_svm <- createDataPartition(df_svm$HealthStatus, p = 0.8, list = FALSE)
train_data_svm <- df_svm[train_index_svm, ]
test_data_svm <- df_svm[-train_index_svm, ]

# Train a Support Vector Machine (SVM) model
model_svm <- svm(HealthStatus ~ . - Year, data = train_data_svm, kernel = "linear")

# Make predictions on the test set
predictions_svm <- predict(model_svm, newdata = test_data_svm)


# Set factor levels explicitly for test_data_svm$HealthStatus
test_data_svm$HealthStatus <- factor(test_data_svm$HealthStatus, levels = c(0, 1))
# Convert both predicted and actual values to factors with the same levels
predictions_svm_factor <- factor(predictions_svm, levels = c(0, 1))
actual_values_factor <- factor(test_data_svm$HealthStatus, levels = c(0, 1))

# Evaluate the SVM model performance
conf_matrix_svm <- confusionMatrix(predictions_svm_factor, actual_values_factor)
print("Confusion Matrix for Support Vector Machine (SVM):")
print(conf_matrix_svm)


```
## Lessons
It also emphasized the need for careful data preparation, feature selection, and model validation. It was critical to address data inconsistencies, select relevant variables, and implement models such as SVM. It became apparent why it is important to interpret model outputs and to improve methods relying on visualizations and assessment criteria.

